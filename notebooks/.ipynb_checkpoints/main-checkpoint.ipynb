{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "013cef88-6fe7-4895-b42c-99a6b107159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application started \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG APP for End Time Message\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Langsmith for tracing\n",
    "print(\"Application started \\n\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://eu.api.smith.langchain.com\"\n",
    "os.environ['USER_AGENT'] = 'MESSAGE_RAG_APP_AGENT'\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76073e82-3c48-4c3d-b0e7-cd8936736c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up chat model\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77264ed8-1b18-4031-9f29-a644585ebeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embeddings model\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0522a268-bd29-422b-981b-4eaf482d7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set up MongoDB connection\n",
    "if not os.environ.get(\"MONGODB_URI\"):\n",
    "    os.environ[\"MONGODB_URI\"] = getpass.getpass(\"Enter your MongoDB connection string: \")\n",
    "\n",
    "if not os.environ.get(\"MONGODB_DATABASE\"):\n",
    "    os.environ[\"MONGODB_DATABASE\"] = getpass.getpass(\"Enter your MongoDB database name: \")\n",
    "\n",
    "if not os.environ.get(\"MONGODB_COLLECTION\"):\n",
    "    os.environ[\"MONGODB_COLLECTION\"] = getpass.getpass(\"Enter your MongoDB collection name: \")\n",
    "\n",
    "if not os.environ.get(\"ATLAS_VECTOR_SEARCH_INDEX_NAME\"):\n",
    "    os.environ[\"ATLAS_VECTOR_SEARCH_INDEX_NAME\"] = getpass.getpass(\"Enter your MongoDB vector search index name: \")\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient(os.environ.get(\"MONGODB_URI\"))\n",
    "db = client[os.environ.get(\"MONGODB_DATABASE\")]\n",
    "collection = db[os.environ.get(\"MONGODB_COLLECTION\")]\n",
    "\n",
    "# Set up vector store\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    embedding=embeddings,\n",
    "    collection=collection,  # Pass the collection object, not a string\n",
    "    index_name=os.environ.get(\"ATLAS_VECTOR_SEARCH_INDEX_NAME\"),\n",
    "    relevance_score_fn=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac000b8-9ecb-4b82-bf0c-fa0e3113bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading documents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finding all pdfs\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Reading PDF for file: /Users/cdetcham/Projects/RAG/MessageSearchApp/data/65/65-0124 Birth Pains VGR.pdf ...\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "from src.document_utilities import find_all_pdfs, load_documents\n",
    "\n",
    "\n",
    "# Load and chunk contents of the sermons\n",
    "data_dir = Path().resolve().parent / \"data\"\n",
    "docs = load_documents(str(data_dir))\n",
    "\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(\"Documents splits are ready to be stored\\n\\n\")\n",
    "\n",
    "# Update metadata (illustration purposes)\n",
    "# total_documents = len(all_splits)\n",
    "# third = total_documents // 3\n",
    "\n",
    "# for i, document in enumerate(all_splits):\n",
    "#     if i < third:\n",
    "#         document.metadata[\"section\"] = \"beginning\"\n",
    "#     elif i < 2 * third:\n",
    "#         document.metadata[\"section\"] = \"middle\"\n",
    "#     else:\n",
    "#         document.metadata[\"section\"] = \"end\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "468511b0-cf22-4879-b1dd-dc4a5b6b9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='BIRTH PAINS' metadata={'source': '/Users/cdetcham/Projects/RAG/MessageSearchApp/data/65/65-0124 Birth Pains VGR.pdf', 'detection_class_prob': 0.6229148507118225, 'coordinates': {'points': ((np.float64(386.5027777777778), np.float64(150.38137817382812)), (np.float64(386.5027777777778), np.float64(213.64116722222218)), (np.float64(694.6928622222222), np.float64(213.64116722222218)), (np.float64(694.6928622222222), np.float64(150.38137817382812))), 'system': 'PixelSpace', 'layout_width': 1088, 'layout_height': 1675}, 'last_modified': '2025-07-14T13:36:30', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': '/Users/cdetcham/Projects/RAG/MessageSearchApp/data/65', 'filename': '65-0124 Birth Pains VGR.pdf', 'category': 'Title', 'element_id': '853a9d14c166cc6287d300759c97b787'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'bulk_write'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Index chunks\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_splits[\u001b[32m0\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m _ = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_splits\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/RAG/MessageSearchApp/.venv/lib/python3.11/site-packages/langchain_mongodb/vectorstores.py:475\u001b[39m, in \u001b[36mMongoDBAtlasVectorSearch.add_documents\u001b[39m\u001b[34m(self, documents, ids, batch_size, **kwargs)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size, n_docs + batch_size, batch_size):\n\u001b[32m    471\u001b[39m     texts, metadatas = \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    472\u001b[39m         *[(doc.page_content, doc.metadata) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents[start:end]]\n\u001b[32m    473\u001b[39m     )\n\u001b[32m    474\u001b[39m     result_ids.extend(\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbulk_embed_and_insert_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     )\n\u001b[32m    479\u001b[39m     start = end\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/RAG/MessageSearchApp/.venv/lib/python3.11/site-packages/langchain_mongodb/vectorstores.py:440\u001b[39m, in \u001b[36mMongoDBAtlasVectorSearch.bulk_embed_and_insert_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids)\u001b[39m\n\u001b[32m    438\u001b[39m operations = [ReplaceOne({\u001b[33m\"\u001b[39m\u001b[33m_id\u001b[39m\u001b[33m\"\u001b[39m: doc[\u001b[33m\"\u001b[39m\u001b[33m_id\u001b[39m\u001b[33m\"\u001b[39m]}, doc, upsert=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m    439\u001b[39m \u001b[38;5;66;03m# insert the documents in MongoDB Atlas\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbulk_write\u001b[49m(operations)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m result.upserted_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [oid_to_str(_id) \u001b[38;5;28;01mfor\u001b[39;00m _id \u001b[38;5;129;01min\u001b[39;00m result.upserted_ids.values()]\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'bulk_write'"
     ]
    }
   ],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "print(\"Completed storage\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d355b-541b-483f-9a9e-b90931a1c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.smith.langchain.com\")\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "    \n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question.\"\n",
    "        \"The content you are provided with are sermons preached by reverend William Marrion Branham,\"\n",
    "        \"sometimes refered to as brother Branham or brother Bill.\"\n",
    "        \"If you don't know the answer, say that you \"\n",
    "        \"don't know. Use five sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "# graph_builder.add_edge(START, \"analyze_query\")\n",
    "# graph = graph_builder.compile()\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0210c2-a59e-4795-8521-cf81fe74c167",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the tree of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_message}]},\n\u001b[1;32m      5\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m      7\u001b[0m ):\n\u001b[1;32m      8\u001b[0m     step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "input_message = \"What is the tree of life?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "\n",
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the messenger of the Ephesian church age?\\n\\n\"\n",
    "    \"Once you get the answer, provide me with the date of the begining and end of the above mentioned church age.\"\n",
    ")\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
